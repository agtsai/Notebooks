{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Develop a system for plagarism detection, where when given a set of textual documents, detect which documents contain significant overlapping content. It is up to you to define what \"significant\" and \"overlapping\" mean. \n",
    "\n",
    "## Methodology\n",
    "\n",
    "I chose to define overlapping content as the overlapping of relatively unique words within a corpus. Significant overlap was chosen to be a degree of overlap that is seen only within 5% of the population.\n",
    "\n",
    "#### Collection\n",
    "I grabbed a small sample of papers from arxiv.org as suggested and extracted the text using PDFMiner.\n",
    "\n",
    "#### Featurization\n",
    "I represented each document as a bag of words (i.e., a dictionary of word counts) and transformed these counts to tf-idf, which weights term frequency by their relative uniqueness to a corpus (and in turn, serves as a better fingerprint of a document). \n",
    "\n",
    "#### Distance metric\n",
    "I chose cosine similarity to measure the \"distance\" between two documents. There are many other distance metrics available, but this is a good start. It normalizes for document length and is easy to understand.\n",
    "\n",
    "#### Algorithm\n",
    "I then computed all\\* pairwise distances between every document, printed a histogram of these values as a sanity check, and then computed the 5% threshold. I then return all document pairs that lie within this threshold.\n",
    "\n",
    "\\* This is an O(n^2) computation that can be ameliorated with some suggestions below.  \n",
    "\n",
    "#### Testing\n",
    "I inserted two test documents into the corpus that were created by sampling from two documents and checked to see if they were identified. \n",
    "\n",
    "## Questions\n",
    "* How would you assess the performance of your system?\n",
    "\n",
    "I would create a test set of labeled documents, and compute the following metrics:\n",
    "\n",
    "Total Accuracy: How many pairs did the system label correctly\n",
    "\n",
    "True Positives (Precision): How accurate were the system's positive labels (aka True Positive)\n",
    "\n",
    "False Positives: How many pairs did the system incorrectly label as being plagiarized\n",
    "\n",
    "True Negatives: How accurate were the system's negative labels\n",
    "\n",
    "False Negatives: How many pairs did the system incorrectly label as being unrelated\n",
    "\n",
    "Recall: How many plagiarized pairs did the system identify out of the total positive population\n",
    "\n",
    "* How could malicious authors potentially fool your system?\n",
    "\n",
    "Stemming should take care of most word conjugations. Malicious actors could replace a large portion of their document with synonyms, insert suble typos to key phrases, and use a different encoding scheme. They could poison the system with very identical documents, thus pushing the 5% tolerance away from the documents they are actually trying to get accepted in the system. \n",
    "\n",
    "* Is your system scalable w.r.t. number of documents / users? \n",
    "If not, how would you address the scalability (in terms of algorithms, infrastructure, or both)?\n",
    "\n",
    "The initial pairwise distance computation would not scale w.r.t. documents. I would ameliorate this by using a k-means algorithm on a large corpus, and doing initial comparisons of new documents on the k-sized set of centroids, and then do comparisons with documents only associated with that cluster. This could be scaled up to a n-level tree of centroids, if necessary. \n",
    "\n",
    "A large enough corpus could not be held completely in memory. At this point, vectorizing the corpus with the bag of words model would require creating a corpus dictionary, and then partitioning the words amongst separate machines, calculating the sub vectors and distances in pieces, then re-assembling them all for a final score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import scoreatpercentile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    'papers/1602.02154.txt',\n",
    "    'papers/1602.02155.txt',\n",
    "    'papers/1602.02160.txt',\n",
    "    'papers/1602.02161.txt',\n",
    "    'papers/1602.02167.txt',\n",
    "    'papers/1602.02183.txt',\n",
    "    'papers/1602.02188.txt',\n",
    "    'papers/1602.02189.txt',\n",
    "    'papers/1602.02213.txt',\n",
    "    'papers/1602.02217.txt',\n",
    "]\n",
    "\n",
    "def make_corpus(filenames):\n",
    "    corpus = []    \n",
    "    for i in filenames:\n",
    "        with codecs.open(i, 'r', 'utf-8') as f:\n",
    "            corpus.append(f.read())    \n",
    "    return corpus\n",
    "\n",
    "def insert_test_documents(corpus, filenames):\n",
    "    \"\"\"\n",
    "    Insert two documents into the corpus that should be identified as plagiarism\n",
    "    \"\"\"\n",
    "    random.seed(\"yewno\")\n",
    "    \n",
    "    doc1 = corpus[0].split()\n",
    "    doc2 = corpus[1].split()\n",
    "    copy1 = \" \".join(random.sample(doc1, len(doc1) / 2) + random.sample(doc2, len(doc2) / 2))\n",
    "    copy2 = \" \".join(random.sample(doc1, len(doc1) / 2) + random.sample(doc2, len(doc2) / 2))\n",
    "    corpus.extend([copy1, copy2])\n",
    "    filenames.extend(['copy1', 'copy2'])\n",
    "    return corpus, filenames\n",
    "\n",
    "def _make_index_pairs(n):\n",
    "    indexes = []\n",
    "    for i in range(0, n):\n",
    "        for j in range(i+1, n):\n",
    "            indexes.append((i,j))\n",
    "    return indexes\n",
    "\n",
    "def get_distance_cutoff(percentile):\n",
    "    return scoreatpercentile(distances, percentile)\n",
    "\n",
    "def get_similar_docs():\n",
    "    cutoff = get_distance_cutoff(5)\n",
    "    print \"Cutoff: {:.2f}\".format(cutoff)\n",
    "    \n",
    "    indexes = _make_index_pairs(len(corpus))\n",
    "    similar_docs = [index for distance, index in zip(distances, indexes)\n",
    "                          if distance < cutoff]\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,   2.,   0.,   8.,  11.,  10.,  10.,  14.,   5.,   3.]),\n",
       " array([ 0.01002143,  0.06251348,  0.11500553,  0.16749758,  0.21998963,\n",
       "         0.27248168,  0.32497373,  0.37746578,  0.42995783,  0.48244988,\n",
       "         0.53494193]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAECCAYAAADq7fyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkgpjsEtSYf4f9oNv7X7KjJMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f792890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get data\n",
    "corpus = make_corpus(filenames)\n",
    "corpus, filenames = insert_test_documents(corpus, filenames)\n",
    "\n",
    "# featurize\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "results = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "# calculate distances \n",
    "distances = pdist(results, 'cosine')\n",
    "plt.hist(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff: 0.09\n",
      "papers/1602.02154.txt and copy1 look similar\n",
      "papers/1602.02154.txt and copy2 look similar\n",
      "papers/1602.02155.txt and copy2 look similar\n",
      "copy1 and copy2 look similar\n"
     ]
    }
   ],
   "source": [
    "# Get similar documents and pretty print them\n",
    "indexes = get_similar_docs()\n",
    "for i in indexes:\n",
    "    first_file = filenames[i[0]]\n",
    "    second_file = filenames[i[1]]\n",
    "    print \"{} and {} look similar\".format(first_file, second_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Assuming that the documents I chose do not have significant overlapping content, but that copy1 and copy2 do, the following document pairs should be identified as likely containing plagiarism:\n",
    "\n",
    "papers/1602.02154.txt, copy1\n",
    "\n",
    "papers/1602.02154.txt, copy2\n",
    "\n",
    "papers/1602.02155.txt, copy1\n",
    "\n",
    "papers/1602.02155.txt, copy2\n",
    "\n",
    "copy1, copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics\n",
      "-------\n",
      "Accuracy: 98.72%\n",
      "True Positives (Precision): 100.00%\n",
      "False Positives: 0.00%\n",
      "True Negatives: 98.65%\n",
      "False Negatives: 1.35%\n",
      "Recall: 80.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the system on the test data and the 5% threshold\n",
    "n = len(corpus)\n",
    "total_pairs = n * (n+1) / 2.0 \n",
    "\n",
    "print \"Metrics\"\n",
    "print \"-------\"\n",
    "print \"Accuracy: {:.2f}%\".format((total_pairs - 1) / total_pairs * 100)\n",
    "print \"True Positives (Precision): {:.2f}%\".format(4.0 / 4.0 * 100)\n",
    "print \"False Positives: {:.2f}%\".format(0.0 / 4.0 * 100)\n",
    "print \"True Negatives: {:.2f}%\".format(73.0 / 74.0 * 100)\n",
    "print \"False Negatives: {:.2f}%\".format(1.0 / 74.0 * 100)\n",
    "print \"Recall: {:.2f}%\".format(4.0 / 5.0 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
